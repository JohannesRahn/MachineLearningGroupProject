{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d40e438-fdb6-47d6-9af7-3fdaaa1d064b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Machine Learning Group Project:** Recommender System Preparation\n",
    "\n",
    "The purpose of this notebook is to extract content-based similarities across different products on the basis of the plethora of of textual data contained in descriptive columns such as _tags_, _genres_, _description_, or _steamspy_tags_. These findings can later be utilized when building other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b0d1d15-a35b-4cfd-b9d0-f8daa3f2a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import requests\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "from kneed import KneeLocator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Parallelization\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1ba9a0-78fb-4de9-b198-6c4562745ea2",
   "metadata": {},
   "source": [
    "## Content-Based Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06cb07-bb8c-4ab2-9347-089b2cc39eef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text Data DataFrame\n",
    "\n",
    "First we create a DataFrame that contains all instances of descriptive textual data for the products. There is a single entry for each game. We run an inner merge because this is the same kind of merge done between these datasets for the final_df in [a_csv_creation.ipynb](a_csv_creation.ipynb). Consequently, this ensures the data we are working with is compatible with other work later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce78a8c-edf1-47f8-b429-f3f2d1405aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the final DataFrame\n",
    "textual_df = pd.read_csv(\"data/final_df.csv\", \n",
    "                       usecols = [\"app_id\",\n",
    "                                  \"categories\", \"genres\",\n",
    "                                  \"steamspy_tags\", \"description\",\n",
    "                                  \"tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05774c4-2b28-49e6-83ff-39662ce03e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_df.drop_duplicates(subset=['app_id'], inplace=True)\n",
    "textual_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "textual_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff163b68-7ce7-46d9-aaf6-8e6c69e387ec",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7a9bf-bac4-4925-be54-bc0c17fcc6fd",
   "metadata": {},
   "source": [
    "#### Text Homogenization\n",
    "\n",
    "The first step undertaken is homgenizing the text across the different columns. We do the following:\n",
    "- Make all words fully lowercase\n",
    "- Transform the _tags_ column from a list to a string\n",
    "- Remove separators in the strings of columns such as _categories_, _steamspy_tags_, etc.\n",
    "- Remove any other punctuation\n",
    "- Homogenize words with spelling discrepancies (e.g. multiplayer & multi-player)\n",
    "\n",
    "Note that we do not remove duplicates (e.g. a game being described as \"Action\" under _genres_ and _steamspy_tags_) because we believe if a word is utilized by more than one source, it should have a stronger weight when vectorizing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdafb4f9-afcd-4b26-9731-7bfeeb6af6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn to String\n",
    "textual_df[\"tags\"] = textual_df[\"tags\"].astype(str)\n",
    "\n",
    "# Remove all punctuation and make everything lowercase\n",
    "textual_df[\"tags\"] = textual_df[\"tags\"].str.translate(str.maketrans('', '', string.punctuation)).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928805b4-a040-4c2c-953a-c3116d8701d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use vectorized operations to remove separators from list columns and turn all to lowercase\n",
    "for col in [\"categories\", \"genres\", \"steamspy_tags\", \"tags\"]:\n",
    "    textual_df[col] = textual_df[col].str.replace(';', ' ').str.lower()\n",
    "\n",
    "    textual_df[col] = textual_df[col].str.replace(\"multi-player\", \"multiplayer\")\\\n",
    "        .str.replace(\"free to play\", \"freetoplay\").str.replace(\"single-player\", \"singleplayer\")\\\n",
    "        .str.replace(\"post-apocalyptic\", \"postapocalyptic\").str.replace(\"scifi\", \"sci-fi\")\\\n",
    "        .str.replace(\"anti cheat\", \"anticheat\").str.replace(\"early access\", \"earlyaccess\")\\\n",
    "        .str.replace(\"shared/split screen\", \"shared/splitscreen\").str.replace(\" &\", \"\")\\\n",
    "        .str.replace(\"sexual content\", \"sexual\").str.replace(\" (require\", \"\").str.replace(\" hl2)\", \"\")\\\n",
    "        .str.replace(\"massively multiplayer\", \"massive-multiplayer\")\\\n",
    "        .str.replace(\"in-app purchases\", \"inapppurcases\").str.replace(\"perma death\", \"permadeath\")\\\n",
    "        .str.replace(\"third person\", \"thirdperson\").str.replace(\"multiple endings\",\"multiending\")\\\n",
    "        .str.replace(\"1990's\", \"1990s\").str.replace(\"25d\", \"2.5d\").str.replace(\"shoot 'em\", \"shootem\")\\\n",
    "        .str.replace(\"co-op\", \"coop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc45ec-1f5f-4219-8a0f-4606c4f01466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define punctuation and numbers\n",
    "punc = '''!()[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "num = '0123456789'\n",
    "\n",
    "# Remove punctuation, numbers, and turn description to lowercase \n",
    "textual_df[\"description\"] = textual_df[\"description\"].str.replace(f'[{punc+num}]', '', regex=True)\\\n",
    "    .str.lower().str.replace(\"multi-player\", \"multiplayer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb145e73-6af4-43ba-b6d3-2ad0e5c97fbb",
   "metadata": {},
   "source": [
    "#### Stopword Removal\n",
    "\n",
    "Although columns like _categories_, _genres_, _steamspy_tags_, and _tags_ do not suffer from this issue, the _description_ column contains stopwords which could be detrimental to our analysis. The stopword list is obtained from a separate [GitHub project](https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a21851d-75ed-4719-acde-b46d478a08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain stopwords\n",
    "stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "stopwords = set(stopwords_list.decode().splitlines())\n",
    "\n",
    "# Remove Stopwords\n",
    "textual_df['description'] = textual_df['description']\\\n",
    ".apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e916c-4414-4539-9852-99ec7a6f00c9",
   "metadata": {},
   "source": [
    "#### Full Text Column Creation\n",
    "\n",
    "Finally, we create a new column which contains all the textual information we have on a game.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c7a6c-026c-4382-900a-11f3d576364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_df['full_text'] = textual_df['categories'] + ' ' + textual_df['genres'] + ' ' + textual_df['steamspy_tags'] + ' ' + textual_df['description'] + ' ' + textual_df['tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2621f0b-e320-4e6e-8bd8-53765ea0935c",
   "metadata": {},
   "source": [
    "### Text Data Exploration\n",
    "\n",
    "We dedicate a section to exploring text data and truly understanding the breadth of data and labels which are available on textual_df."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3673fc12-59ed-49cd-a556-12b0a568ccc6",
   "metadata": {},
   "source": [
    "**Number of Distinct Labels:** All four labelling columns feature different tag systems with wholly different labels (and variety thereof). Here we check how these vary in terms of numbers. While _categories_ and _genres_ have a relatively restrained count of unique labels (41 and 25 respectively), _tags_ and _steamspy_tags_ have much greater breadth (with 479 and 373 respectively). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f790161-e4db-4203-8e3d-fd12eee6f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"categories\", \"genres\", \"steamspy_tags\", \"tags\"]:\n",
    "    split_count = len(textual_df[i].str.split(' ', expand=True).stack().unique())\n",
    "    print(f\"The \\033[1m'{i}' column\\033[0m features a total of \\033[1m{split_count} distinct labels\\033[0m.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33fa71a-b3c5-4e91-bf81-56c2c0c46b74",
   "metadata": {},
   "source": [
    "**Average Number of Labels per Game:** While there is a great difference in the number of unique labels, the average number of labels assigned to a single game is on average quite stable. In fact, _genres_, _steamspy_tags_, and _tags_ all approximately have the same number of labels per row (to two decimal figures). Despite having the second lowest number of different labels, _categories_ has the highest number of labels per row: 8.12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4833ab-cf05-420a-96c1-ccf8f3e468cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"categories\", \"genres\", \"steamspy_tags\", \"tags\"]:\n",
    "    \n",
    "    split = [len(j) for j in textual_df[i].str.split(' ', expand=True).stack()]\n",
    "    split_mean = sum(split) / len(split)\n",
    "    \n",
    "    print(\"The entries in the\\033[1m\", f\"'{i}'\", \"column\\033[0m have an approximate average of\\033[1m\", \n",
    "          round(split_mean,2), \"labels per row\\033[0m.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ab1950-4862-4835-8086-ce29d8a39db2",
   "metadata": {},
   "source": [
    "**Most Common Labels:** AAAAA WE INVESTIGATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5911b-a011-4896-83de-2c03afe7175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"categories\", \"genres\", \"steamspy_tags\", \"tags\"]:\n",
    "    \n",
    "    split_count = textual_df[i].str.split(' ', expand=True).stack().value_counts()\n",
    "    \n",
    "    print(\"The most frequently recurring tags (along with their occurrence) in the\", f\"'{i}'\",\n",
    "          \"column:\", f\"\\n{split_count.nlargest(10)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339a4ba2-2db7-4321-90f2-6680ec2de41b",
   "metadata": {},
   "source": [
    "**Tag Intersection:** Finally, we check if there are certain tags which exist in multiple columns (e.g. \"action\" being an entry in the _genres_, but also in _tags_. We find that there are AAAAAAAAAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a3496-103b-4299-bc70-bb31eeecb4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in each column\n",
    "categories = set(textual_df['categories'].str.split(' ').explode().str.strip())\n",
    "tags = set(textual_df['tags'].str.split(' ').explode().str.strip())\n",
    "steamspy_tags = set(textual_df['steamspy_tags'].str.split(' ').explode().str.strip())\n",
    "genre = set(textual_df['genres'].str.split(' ').explode().str.strip())\n",
    "\n",
    "# Find the intersection of the sets to get the values that exist in all four columns\n",
    "or_intersection = categories | tags | steamspy_tags | genre\n",
    "and_intersection = categories & tags & steamspy_tags & genre\n",
    "\n",
    "# Print the number of unique values that exist in all four columns\n",
    "print(\"The number of labels present in at least two columns is:\", len(or_intersection))\n",
    "print(\"The number of labels present in all four columns is:\", len(and_intersection))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da676c-f2aa-4d77-8fcb-94e9e84580e9",
   "metadata": {},
   "source": [
    "#### Textual Data Vectorization\n",
    "\n",
    "First we create a vector containing information concerning the textual descriptions of the games. To this end, TF vectorization through TfidfVectorizer. Note that we purposely decide not to make use of idf. We do this on the basis of the notion that that if a certain label is common in a game's metadata across multiple sources which tag the games, it is clearly more appropriate to describe the game than the others. We remove IDF because of the imbalance in labels across sources. Labels which are shared across sources are likely to be the more common and recognizable ones utilized on the platform. They would be the ones to be penalized, in favor of very niche terms in the much larger  By maintaining IDF, it is likely that these more descriptive terms would be lost in favor of more niche terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d0961b-a3fe-44ca-a5b3-140433590659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words = \"english\",\n",
    "                            strip_accents = \"ascii\",\n",
    "                            use_idf = False)\n",
    "\n",
    "# Fit and transform full text column\n",
    "fulltext_vector = vectorizer.fit_transform(textual_df['full_text'])\n",
    "fulltext_array = fulltext_vector.toarray()\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = vectorizer.vocabulary_.keys()\n",
    "\n",
    "# Create DataFrame\n",
    "vector_df = pd.DataFrame(fulltext_array, columns = feature_names,\n",
    "                        index = textual_df[\"app_id\"])\n",
    "vector_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7733d1b1-958a-4836-a5ab-409555c092c5",
   "metadata": {},
   "source": [
    "#### Optimal Component Selection: Scree Test\n",
    "\n",
    "The ultimate objective is to utilize Truncated Singular Value Decomposition to reduce the textual data's dimensionality and create a latent matrix. In order to do that, we conduct a Scree Test to determine the optimal number of components to be used. We decide that we want at least 90% of variance being explained by the decomposition. We find the minimum number of components necessary to achieve this and use the elbow method to identify the optimal number of components under the constraint that it explains at least 90% of data.\n",
    "\n",
    "In order to identify the knee point, we use the [_kneed_](https://pypi.org/project/kneed/) library, which is based on the [_kneedle algorithm_](https://raghavan.usc.edu//papers/kneedle-simplex11.pdf) developed by Ville Satopaa, Jeannie Albrecht, David Irwin, and Barath Raghavan. \n",
    "\n",
    "**Note:** Fitting the svd model requires a lot of time. It is advised to only run the block of code below only once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8592fed-f02d-431c-b847-f6f3a7a14b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Instance of SVD with 300 Components\n",
    "svd = TruncatedSVD(n_components=300, random_state = 70)\n",
    "\n",
    "# Fit to the Matrix\n",
    "scree_matrix = svd.fit(vector_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6502c032-cde6-40b7-99a0-aae9ac3055bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Cumulative Explained Variance Percentage\n",
    "cum_variance = np.cumsum(scree_matrix.explained_variance_ratio_)\n",
    "\n",
    "# Identify First Instance of Expl. Variance Above 90%\n",
    "ninety_threshold = np.argmax(cum_variance >= 0.9) \n",
    "\n",
    "print(f\"The number of features required to explain at least 90% of variance is {ninety_threshold + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9407e6e-09db-4909-a090-f6f2fde2153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain Explained Variance\n",
    "expl_variance = scree_matrix.explained_variance_\n",
    "\n",
    "# Find Elbow Point\n",
    "kneedle = KneeLocator(np.arange(ninety_threshold + 1, len(expl_variance)+ 1), \n",
    "                      expl_variance[ninety_threshold:], curve='convex', \n",
    "                      direction='decreasing')\n",
    "\n",
    "elbow_location = kneedle.elbow\n",
    "\n",
    "# Plot Elbow\n",
    "sns.set_theme()\n",
    "\n",
    "plt.plot(np.arange(1, len(expl_variance)+1),\n",
    "         expl_variance[0:],\n",
    "         label = \"Explained Variance\")\n",
    "\n",
    "# Add Title and Axis Labels\n",
    "plt.title(\"Scree Test for SVD Analysis\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.legend()\n",
    "\n",
    "# Add Annotation and Point to Show Location of 90%\n",
    "plt.annotate(f\"90% of Variance Explained at {ninety_threshold + 1} Components\", \n",
    "             xy=(ninety_threshold + 1, expl_variance[ninety_threshold]),\n",
    "             xytext=(40, 0.010),\n",
    "             arrowprops=dict(arrowstyle=\"-\",\n",
    "                             color = \"blue\"), fontsize=10)\n",
    "\n",
    "plt.plot(ninety_threshold + 1, expl_variance[ninety_threshold], \"bo\",\n",
    "         label = \"Elbow Point\")\n",
    "\n",
    "# Add Annotation and Point to Show Location of Elbow\n",
    "plt.annotate(f\"Post-90% Elbow at {elbow_location} Components\", \n",
    "             xy=(elbow_location, expl_variance[elbow_location - 1]),\n",
    "             xytext=(200, 0.015),\n",
    "             arrowprops=dict(arrowstyle=\"-\",\n",
    "                             color = \"red\"), fontsize=10)\n",
    "\n",
    "plt.plot(elbow_location, expl_variance[elbow_location - 1], \"ro\",\n",
    "         label = \"Elbow Point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1ef764-9978-4ffb-b3e7-cc2395685f75",
   "metadata": {},
   "source": [
    "#### Optimized SVD\n",
    "\n",
    "Plotting the explained variance against the number of SVD components, we find that 90% of variance is in fact explained far past the plot's actual elbow. Finding the \"post-90%\" elbow and identifying it on the plot shows that it is located on the tail of the plot. It also consists of only 4 components more than the 90% variance point. Consequently, we decide to stick to our minimum variance point of 156 components with at least 90% variance as opposed to the 160 of the post-90% elbow. We believe the returns to making the model more complex are not sufficient.\n",
    "\n",
    "We run the truncated SVD with 156 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c291f49a-8e34-4b2a-a3be-f8eac0106375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Instance of SVD with 160 Components\n",
    "svd_opt = TruncatedSVD(n_components=160, random_state = 70)\n",
    "\n",
    "# Fit to the Matrix\n",
    "lat_matrix = svd_opt.fit_transform(vector_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2e73d-5df9-4cc9-a470-a82ad61f20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn Into DataFrame\n",
    "lat_df = pd.DataFrame(lat_matrix)\n",
    "\n",
    "# Add Movie Labels\n",
    "titles_df = pd.concat([pd.Series(vector_df.index), lat_df], axis = 1)\n",
    "\n",
    "# Cosine Similarity\n",
    "cos_similarity = cosine_similarity(titles_df.iloc[:, 1:])\n",
    "\n",
    "content_similarity_df = pd.DataFrame(cos_similarity, index = titles_df[\"app_id\"], \n",
    "             columns = titles_df[\"app_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3eff59-5f8b-484f-89ba-64ab8908e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_similarity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f17a751-7572-4fe2-9bf0-c8e859fe34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to file\n",
    "content_similarity_df.to_csv('data/content_base.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e06de9-a059-4339-b887-12cc6ae3b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Unnecessary DataFrames\n",
    "del textual_df\n",
    "del svd\n",
    "del lat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c91e26-646f-44d5-a001-ba0827587f7b",
   "metadata": {},
   "source": [
    "## User Similarity\n",
    "\n",
    "Now we create a DataFrame that maps the similarity across users. We do this based on a latent matrix mapping their ratings on individual Steam games. More specifically, we look to find the similarity between all those users which provide a single review and those which provide more than one. This will be useful later for the creation of a similarity column.\n",
    "\n",
    "### Import DataFrame\n",
    "\n",
    "We import the dataframe with the y, app_id, and user_id. We replace 0s with -1s in order to clearly denote a negative review. We also create two DataFrames, one with users with who have only one review, and one with users who have more than one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1efe9d92-bbbc-452d-9714-640f378b79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the user DataFrame\n",
    "user_df = pd.read_csv(\"data/final_df.csv\", \n",
    "                       usecols = [\"app_id\", \"user_id\",\n",
    "                                 \"y\"])\n",
    "\n",
    "# Turn Booleans to Integers\n",
    "user_df[\"y\"] = user_df[\"y\"].astype(int).replace(0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ec60f8-fc92-4fa8-b2d4-36d5117b4bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Datapoints Lost\n"
     ]
    }
   ],
   "source": [
    "# Create DF for users with only 1 review and more than 1 \n",
    "duplicated_user_ids = user_df[user_df.duplicated(subset=[\"user_id\"], keep=False)][\"user_id\"].unique()\n",
    "multi_review = user_df[user_df[\"user_id\"].isin(duplicated_user_ids)].reset_index(drop=True)\n",
    "\n",
    "duplicated_user_ids = user_df[user_df.duplicated(subset=[\"user_id\"], keep=False)][\"user_id\"].unique()\n",
    "single_review = user_df[user_df[\"user_id\"].isin(set(user_df[\"user_id\"]) - set(duplicated_user_ids))].reset_index(drop=True)\n",
    "\n",
    "# print the resulting dataframe\n",
    "if (len(user_df) == (len(multi_review) + len(single_review))):\n",
    "    print(\"No Datapoints Lost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7eae69-e0f4-417c-99af-ed49b1ec5c65",
   "metadata": {},
   "source": [
    "### Create Pivot Table and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f76a30bb-a2f9-4c02-a559-ff6c0e4194d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pivot Table\n",
    "rec_pivot = user_df.pivot_table(index = \"user_id\", columns = \"app_id\",\n",
    "                   values = \"y\")\n",
    "\n",
    "# Fill NaNs\n",
    "rec_pivot = rec_pivot.fillna(0)\n",
    "\n",
    "# Convert DataFrame to Sparse Matrix\n",
    "sparse_recs = csr_matrix(rec_pivot.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6621c312-5f3d-43f1-a577-8175a2de871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Instance of SVD with 250 Components\n",
    "user_svd = TruncatedSVD(n_components=250, random_state = 70)\n",
    "\n",
    "# Fit to the Matrix\n",
    "user_svd_fit = user_svd.fit(sparse_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bec7d4-ddf0-46d6-a462-8cf5ac9bb066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Cumulative Explained Variance Percentage\n",
    "user_cum_variance = np.cumsum(user_svd_fit.explained_variance_ratio_)\n",
    "\n",
    "# Identify First Instance of Expl. Variance Above 90%\n",
    "user_ninety_threshold = np.argmax(user_cum_variance >= 0.9) \n",
    "\n",
    "print(f\"The number of features required to explain at least 90% of variance is {user_ninety_threshold + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8888bb9-fc0f-4574-a9bb-a9ad42cc53c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain Explained Variance\n",
    "user_expl_variance = user_svd_fit.explained_variance_\n",
    "\n",
    "# Find Elbow Point\n",
    "user_kneedle = KneeLocator(np.arange(user_ninety_threshold + 1, len(user_expl_variance)+ 1), \n",
    "                      expl_variance[user_ninety_threshold:], curve='convex', \n",
    "                      direction='decreasing')\n",
    "\n",
    "user_elbow_location = user_kneedle.elbow\n",
    "\n",
    "# Plot Elbow\n",
    "sns.set_theme()\n",
    "\n",
    "plt.plot(np.arange(1, len(user_expl_variance)+1),\n",
    "         user_expl_variance[0:],\n",
    "         label = \"Explained Variance\")\n",
    "\n",
    "# Add Title and Axis Labels\n",
    "plt.title(\"Scree Test for SVD Analysis\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.legend()\n",
    "\n",
    "# Add Annotation and Point to Show Location of 90%\n",
    "plt.annotate(f\"90% of Variance Explained at {user_ninety_threshold + 1} Components\", \n",
    "             xy=(user_ninety_threshold + 1, user_expl_variance[user_ninety_threshold]),\n",
    "             xytext=(40, 0.010),\n",
    "             arrowprops=dict(arrowstyle=\"-\",\n",
    "                             color = \"blue\"), fontsize=10)\n",
    "\n",
    "plt.plot(user_ninety_threshold + 1, user_expl_variance[user_ninety_threshold], \"bo\",\n",
    "         label = \"Elbow Point\")\n",
    "\n",
    "# Add Annotation and Point to Show Location of Elbow\n",
    "plt.annotate(f\"Post-90% Elbow at {user_elbow_location} Components\", \n",
    "             xy=(user_elbow_location, user_expl_variance[user_elbow_location - 1]),\n",
    "             xytext=(200, 0.015),\n",
    "             arrowprops=dict(arrowstyle=\"-\",\n",
    "                             color = \"red\"), fontsize=10)\n",
    "\n",
    "plt.plot(user_elbow_location, user_expl_variance[user_elbow_location - 1], \"ro\",\n",
    "         label = \"Elbow Point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2152fcba-1a93-4135-8309-410af424ee26",
   "metadata": {},
   "source": [
    "### Create Latent Matrix and Find Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812433b2-aeed-474c-a556-7f3d1ec0fdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# Transform Data into Latent Matrix\n",
    "user_latent = pd.DataFrame(user_svd_fit.transform(sparse_recs),\n",
    "                          index = rec_pivot.index)\n",
    "user_latent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45268fbf-858e-421d-9739-1d84cc3af1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cosine similarity between single-review and multi-review\n",
    "row_users = single_review[\"user_id\"].unique()\n",
    "col_users = multi_review[\"user_id\"].unique()\n",
    "\n",
    "# Only take the optimal number of dimensions\n",
    "single_review_latent = transformed_recs[row_users, :n]\n",
    "user_lat_matrix = transformed_recs[col_users, :n]\n",
    "\n",
    "# Compute the Cosine Similarity Matrix Between Rows of the Latent Matrix for the Selected User IDs\n",
    "user_similarity_matrix = cosine_similarity(single_review_latent, user_lat_matrix)\n",
    "\n",
    "# Create a DataFrame with the similarity matrix, indexed by row and column user IDs\n",
    "user_similarity_df = pd.DataFrame(similarity_matrix, index=row_users, columns=col_users,\n",
    "                                 index = row_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f4bfd758-fcb7-496d-a0a7-1755c6ec5b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ed061-8a4a-4c9a-b4f5-83ad63b4d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_lat_matrix\n",
    "del single_review_latent\n",
    "del row_users\n",
    "del col_users\n",
    "del user_latent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c29fe2-94fe-4e8c-94f0-3d304a994dc2",
   "metadata": {},
   "source": [
    "## Feature Creation\n",
    "\n",
    "Finally, we create a brand new feature in our final_df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d1cbb060-e6a4-430a-a0fa-8b8e6b102249",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_similarity_df = pd.read_csv(\"data/content_base.csv\", index_col = \"app_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5c00df28-3ba8-4a3e-9e21-14c9a02ecb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>app_id</th>\n",
       "      <th>304390</th>\n",
       "      <th>306130</th>\n",
       "      <th>238960</th>\n",
       "      <th>730</th>\n",
       "      <th>255710</th>\n",
       "      <th>289070</th>\n",
       "      <th>431960</th>\n",
       "      <th>635260</th>\n",
       "      <th>392160</th>\n",
       "      <th>570</th>\n",
       "      <th>...</th>\n",
       "      <th>489520</th>\n",
       "      <th>555950</th>\n",
       "      <th>394510</th>\n",
       "      <th>410320</th>\n",
       "      <th>285920</th>\n",
       "      <th>17470</th>\n",
       "      <th>403640</th>\n",
       "      <th>314160</th>\n",
       "      <th>704850</th>\n",
       "      <th>485510</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304390</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.394583</td>\n",
       "      <td>0.652058</td>\n",
       "      <td>0.480446</td>\n",
       "      <td>0.204640</td>\n",
       "      <td>0.360051</td>\n",
       "      <td>0.138724</td>\n",
       "      <td>0.433073</td>\n",
       "      <td>0.259101</td>\n",
       "      <td>0.490285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530043</td>\n",
       "      <td>0.301991</td>\n",
       "      <td>0.514234</td>\n",
       "      <td>0.519567</td>\n",
       "      <td>0.406684</td>\n",
       "      <td>0.331687</td>\n",
       "      <td>0.332657</td>\n",
       "      <td>0.335709</td>\n",
       "      <td>0.301892</td>\n",
       "      <td>0.601789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306130</th>\n",
       "      <td>0.394583</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.619674</td>\n",
       "      <td>0.269352</td>\n",
       "      <td>0.157970</td>\n",
       "      <td>0.178135</td>\n",
       "      <td>0.111520</td>\n",
       "      <td>0.387001</td>\n",
       "      <td>0.246213</td>\n",
       "      <td>0.332852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304212</td>\n",
       "      <td>0.212217</td>\n",
       "      <td>0.138190</td>\n",
       "      <td>0.220257</td>\n",
       "      <td>0.471072</td>\n",
       "      <td>0.198846</td>\n",
       "      <td>0.326764</td>\n",
       "      <td>0.343884</td>\n",
       "      <td>0.307128</td>\n",
       "      <td>0.509522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238960</th>\n",
       "      <td>0.652058</td>\n",
       "      <td>0.619674</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.443977</td>\n",
       "      <td>0.166088</td>\n",
       "      <td>0.248202</td>\n",
       "      <td>0.171930</td>\n",
       "      <td>0.434388</td>\n",
       "      <td>0.245030</td>\n",
       "      <td>0.554080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467581</td>\n",
       "      <td>0.314532</td>\n",
       "      <td>0.415626</td>\n",
       "      <td>0.431764</td>\n",
       "      <td>0.416878</td>\n",
       "      <td>0.274098</td>\n",
       "      <td>0.311742</td>\n",
       "      <td>0.285849</td>\n",
       "      <td>0.275666</td>\n",
       "      <td>0.731631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>0.480446</td>\n",
       "      <td>0.269352</td>\n",
       "      <td>0.443977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.277238</td>\n",
       "      <td>0.355281</td>\n",
       "      <td>0.245533</td>\n",
       "      <td>0.410403</td>\n",
       "      <td>0.312661</td>\n",
       "      <td>0.548046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.275654</td>\n",
       "      <td>0.472461</td>\n",
       "      <td>0.507455</td>\n",
       "      <td>0.364353</td>\n",
       "      <td>0.190225</td>\n",
       "      <td>0.323074</td>\n",
       "      <td>0.290928</td>\n",
       "      <td>0.219978</td>\n",
       "      <td>0.474242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255710</th>\n",
       "      <td>0.204640</td>\n",
       "      <td>0.157970</td>\n",
       "      <td>0.166088</td>\n",
       "      <td>0.277238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.440692</td>\n",
       "      <td>0.280810</td>\n",
       "      <td>0.287209</td>\n",
       "      <td>0.519933</td>\n",
       "      <td>0.248412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254262</td>\n",
       "      <td>0.320823</td>\n",
       "      <td>0.127167</td>\n",
       "      <td>0.230673</td>\n",
       "      <td>0.312222</td>\n",
       "      <td>0.096302</td>\n",
       "      <td>0.298973</td>\n",
       "      <td>0.288785</td>\n",
       "      <td>0.371059</td>\n",
       "      <td>0.302191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17470</th>\n",
       "      <td>0.331687</td>\n",
       "      <td>0.198846</td>\n",
       "      <td>0.274098</td>\n",
       "      <td>0.190225</td>\n",
       "      <td>0.096302</td>\n",
       "      <td>0.069726</td>\n",
       "      <td>0.103804</td>\n",
       "      <td>0.121625</td>\n",
       "      <td>0.292479</td>\n",
       "      <td>0.125171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092875</td>\n",
       "      <td>0.476745</td>\n",
       "      <td>0.271316</td>\n",
       "      <td>0.389122</td>\n",
       "      <td>0.298862</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.311108</td>\n",
       "      <td>0.174169</td>\n",
       "      <td>0.273764</td>\n",
       "      <td>0.297453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403640</th>\n",
       "      <td>0.332657</td>\n",
       "      <td>0.326764</td>\n",
       "      <td>0.311742</td>\n",
       "      <td>0.323074</td>\n",
       "      <td>0.298973</td>\n",
       "      <td>0.184740</td>\n",
       "      <td>0.244294</td>\n",
       "      <td>0.226281</td>\n",
       "      <td>0.371855</td>\n",
       "      <td>0.219213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187007</td>\n",
       "      <td>0.424635</td>\n",
       "      <td>0.161784</td>\n",
       "      <td>0.373376</td>\n",
       "      <td>0.358941</td>\n",
       "      <td>0.311108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.201534</td>\n",
       "      <td>0.387046</td>\n",
       "      <td>0.463447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314160</th>\n",
       "      <td>0.335709</td>\n",
       "      <td>0.343884</td>\n",
       "      <td>0.285849</td>\n",
       "      <td>0.290928</td>\n",
       "      <td>0.288785</td>\n",
       "      <td>0.315979</td>\n",
       "      <td>0.097088</td>\n",
       "      <td>0.465208</td>\n",
       "      <td>0.406012</td>\n",
       "      <td>0.240811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272634</td>\n",
       "      <td>0.169885</td>\n",
       "      <td>0.232270</td>\n",
       "      <td>0.270962</td>\n",
       "      <td>0.386592</td>\n",
       "      <td>0.174169</td>\n",
       "      <td>0.201534</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.443385</td>\n",
       "      <td>0.286356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704850</th>\n",
       "      <td>0.301892</td>\n",
       "      <td>0.307128</td>\n",
       "      <td>0.275666</td>\n",
       "      <td>0.219978</td>\n",
       "      <td>0.371059</td>\n",
       "      <td>0.241019</td>\n",
       "      <td>0.176856</td>\n",
       "      <td>0.298693</td>\n",
       "      <td>0.444373</td>\n",
       "      <td>0.211003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212067</td>\n",
       "      <td>0.276286</td>\n",
       "      <td>0.193237</td>\n",
       "      <td>0.255163</td>\n",
       "      <td>0.438764</td>\n",
       "      <td>0.273764</td>\n",
       "      <td>0.387046</td>\n",
       "      <td>0.443385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.291959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485510</th>\n",
       "      <td>0.601789</td>\n",
       "      <td>0.509522</td>\n",
       "      <td>0.731631</td>\n",
       "      <td>0.474242</td>\n",
       "      <td>0.302191</td>\n",
       "      <td>0.312539</td>\n",
       "      <td>0.251487</td>\n",
       "      <td>0.365080</td>\n",
       "      <td>0.373880</td>\n",
       "      <td>0.436339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.390169</td>\n",
       "      <td>0.408973</td>\n",
       "      <td>0.370323</td>\n",
       "      <td>0.491454</td>\n",
       "      <td>0.394400</td>\n",
       "      <td>0.297453</td>\n",
       "      <td>0.463447</td>\n",
       "      <td>0.286356</td>\n",
       "      <td>0.291959</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows × 311 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "app_id    304390    306130    238960    730       255710    289070    431960   \n",
       "app_id                                                                         \n",
       "304390  1.000000  0.394583  0.652058  0.480446  0.204640  0.360051  0.138724  \\\n",
       "306130  0.394583  1.000000  0.619674  0.269352  0.157970  0.178135  0.111520   \n",
       "238960  0.652058  0.619674  1.000000  0.443977  0.166088  0.248202  0.171930   \n",
       "730     0.480446  0.269352  0.443977  1.000000  0.277238  0.355281  0.245533   \n",
       "255710  0.204640  0.157970  0.166088  0.277238  1.000000  0.440692  0.280810   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "17470   0.331687  0.198846  0.274098  0.190225  0.096302  0.069726  0.103804   \n",
       "403640  0.332657  0.326764  0.311742  0.323074  0.298973  0.184740  0.244294   \n",
       "314160  0.335709  0.343884  0.285849  0.290928  0.288785  0.315979  0.097088   \n",
       "704850  0.301892  0.307128  0.275666  0.219978  0.371059  0.241019  0.176856   \n",
       "485510  0.601789  0.509522  0.731631  0.474242  0.302191  0.312539  0.251487   \n",
       "\n",
       "app_id    635260    392160    570     ...    489520    555950    394510   \n",
       "app_id                                ...                                 \n",
       "304390  0.433073  0.259101  0.490285  ...  0.530043  0.301991  0.514234  \\\n",
       "306130  0.387001  0.246213  0.332852  ...  0.304212  0.212217  0.138190   \n",
       "238960  0.434388  0.245030  0.554080  ...  0.467581  0.314532  0.415626   \n",
       "730     0.410403  0.312661  0.548046  ...  0.445600  0.275654  0.472461   \n",
       "255710  0.287209  0.519933  0.248412  ...  0.254262  0.320823  0.127167   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "17470   0.121625  0.292479  0.125171  ...  0.092875  0.476745  0.271316   \n",
       "403640  0.226281  0.371855  0.219213  ...  0.187007  0.424635  0.161784   \n",
       "314160  0.465208  0.406012  0.240811  ...  0.272634  0.169885  0.232270   \n",
       "704850  0.298693  0.444373  0.211003  ...  0.212067  0.276286  0.193237   \n",
       "485510  0.365080  0.373880  0.436339  ...  0.390169  0.408973  0.370323   \n",
       "\n",
       "app_id    410320    285920    17470     403640    314160    704850    485510  \n",
       "app_id                                                                        \n",
       "304390  0.519567  0.406684  0.331687  0.332657  0.335709  0.301892  0.601789  \n",
       "306130  0.220257  0.471072  0.198846  0.326764  0.343884  0.307128  0.509522  \n",
       "238960  0.431764  0.416878  0.274098  0.311742  0.285849  0.275666  0.731631  \n",
       "730     0.507455  0.364353  0.190225  0.323074  0.290928  0.219978  0.474242  \n",
       "255710  0.230673  0.312222  0.096302  0.298973  0.288785  0.371059  0.302191  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "17470   0.389122  0.298862  1.000000  0.311108  0.174169  0.273764  0.297453  \n",
       "403640  0.373376  0.358941  0.311108  1.000000  0.201534  0.387046  0.463447  \n",
       "314160  0.270962  0.386592  0.174169  0.201534  1.000000  0.443385  0.286356  \n",
       "704850  0.255163  0.438764  0.273764  0.387046  0.443385  1.000000  0.291959  \n",
       "485510  0.491454  0.394400  0.297453  0.463447  0.286356  0.291959  1.000000  \n",
       "\n",
       "[311 rows x 311 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271255a-8750-4df5-b635-1b7f89bd532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Generator\n",
    "class ContentSimilarityGenerator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, users, similarity):\n",
    "        self.users = users\n",
    "        self.similarity = similarity\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        similarities = []\n",
    "        for i in range(0, len(X)):\n",
    "            user = X.at[i, 'user_id']\n",
    "            game = X.at[i, 'app_id']\n",
    "            sim = self.AverageSimilarityCalculator(user, game)\n",
    "            similarities.append(sim)\n",
    "        return similarities\n",
    "    \n",
    "    def AverageSimilarityCalculator(self, user, game):\n",
    "        user_ratings = self.users[self.users[\"user_id\"] == user]\n",
    "        other_games = user_ratings[user_ratings['app_id'] != game]\n",
    "        other_games_titles = other_games['app_id']\n",
    "        filtered_game_similarity_df = self.similarity.loc[game, other_games_titles] * other_games.set_index('app_id')[\"y\"]\n",
    "        return filtered_game_similarity_df.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80e41af5-2543-4a0b-ac39-43b18a132f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Average Similarity\n",
    "def AverageSimilarityCalculator(user, game):\n",
    "    \n",
    "    user_ratings = multi_review[multi_review[\"user_id\"] == user]\n",
    "    other_games = user_ratings[user_ratings['app_id'] != game]\n",
    "    \n",
    "    other_games_titles = other_games['app_id']\n",
    "    filtered_game_similarity_df = content_similarity_df.loc[game, other_games_titles] * other_games.set_index('app_id')[\"y\"]\n",
    "    \n",
    "    return filtered_game_similarity_df.mean()\n",
    "\n",
    "# Create Compiler of Avg Sim\n",
    "def SimilarityGenerator(users = multi_review, \n",
    "                     similarity = content_similarity_df):\n",
    "    \n",
    "    similarities = []\n",
    "    for i in range(0, len(users)):\n",
    "        \n",
    "        sim = AverageSimilarityCalculator(users.at[i, 'user_id'],\n",
    "                      users.at[i, 'app_id'])\n",
    "        \n",
    "        similarities.append(sim)\n",
    "    \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "31c981e4-9368-467a-9dfb-56afd5fd2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Most Similary User\n",
    "def UserPairer(user,\n",
    "              user_similarity = user_similarity_df):\n",
    "    \n",
    "    return user_similarity_df.loc[user].idxmax()\n",
    "    \n",
    "# Create Similarity Imputer\n",
    "def SimilarityImputer(users = single_review):\n",
    "    \n",
    "    sim_list = []\n",
    "    for i in range(0, len(users)):\n",
    "        similar_user = UserPairer(users.at[i, \"user_id\"])\n",
    "        \n",
    "        sim = AverageSimilarityCalculator(similar_user,\n",
    "                                   users.at[i, \"app_id\"])\n",
    "        \n",
    "        sim_list.append(sim_list)\n",
    "    \n",
    "    return sim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8cc9878c-ed17-4bad-bc83-6e271f030007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Similarity Generator\n",
    "multi_similarities = SimilarityGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d5d89ba-f41e-43b1-9b58-3a0ef5e8206c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>app_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>306130</td>\n",
       "      <td>17622</td>\n",
       "      <td>0.500506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>255710</td>\n",
       "      <td>125959</td>\n",
       "      <td>0.279530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>431960</td>\n",
       "      <td>72938</td>\n",
       "      <td>0.068579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>635260</td>\n",
       "      <td>29571</td>\n",
       "      <td>-0.625352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>392160</td>\n",
       "      <td>144843</td>\n",
       "      <td>-0.567523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3489009</th>\n",
       "      <td>1</td>\n",
       "      <td>225540</td>\n",
       "      <td>2894100</td>\n",
       "      <td>0.258590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3489010</th>\n",
       "      <td>1</td>\n",
       "      <td>225540</td>\n",
       "      <td>4252049</td>\n",
       "      <td>0.561824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3489011</th>\n",
       "      <td>1</td>\n",
       "      <td>225540</td>\n",
       "      <td>2843922</td>\n",
       "      <td>0.569846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3489012</th>\n",
       "      <td>1</td>\n",
       "      <td>225540</td>\n",
       "      <td>2173819</td>\n",
       "      <td>0.466156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3489013</th>\n",
       "      <td>1</td>\n",
       "      <td>225540</td>\n",
       "      <td>1387304</td>\n",
       "      <td>0.398844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3489014 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         y  app_id  user_id  similarities\n",
       "0        1  306130    17622      0.500506\n",
       "1        1  255710   125959      0.279530\n",
       "2        1  431960    72938      0.068579\n",
       "3        1  635260    29571     -0.625352\n",
       "4       -1  392160   144843     -0.567523\n",
       "...     ..     ...      ...           ...\n",
       "3489009  1  225540  2894100      0.258590\n",
       "3489010  1  225540  4252049      0.561824\n",
       "3489011  1  225540  2843922      0.569846\n",
       "3489012  1  225540  2173819      0.466156\n",
       "3489013  1  225540  1387304      0.398844\n",
       "\n",
       "[3489014 rows x 4 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create New Column with Similarities for multi_review\n",
    "multi_review[\"similarities\"] = multi_similarities\n",
    "\n",
    "multi_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b89d33e8-53f1-4531-bd67-a6010bafa868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to file\n",
    "multi_review.to_csv('data/multi_review.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c65ba-97e4-4af6-8b0c-f54523d01f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_similarities = SimilarityImputer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
